\chapter{Grundlagen}
Wie in \autoref{sec:einleitung} bereits angedeutet, werden in dieser Arbeit Methoden aus dem Bereich der künstlichen Intelligenz verwendet, um das gestellte Problem zu lösen.
Nachfolgend werden die wichtigsten Grundlagen zum Verständnis dieser Ausarbeitung erläutert.
Weiterhin wird kurz auf die zur Simulation verwendete Software eingegangen.
Im Rahmen der konzeptuellen Planung wird auch von mathematischen Grundlagen des dreidimensionalen Raums Gebrauch gemacht, welche deshalb ebenfalls kurz beleuchtet werden sollen.

\section{Machine Learning}
Machine Learning ist eine Unterkategorie der künstlichen Intelligenz und bezeichnet einen automatisierten Prozess, der es Computern ermöglicht, eigenständig aus Trainingsdaten zu lernen und sich mit der Zeit zu verbessern, ohne explizit zur Lösung einer Aufgabe programmiert zu werden.
Machine Learning Algorithmen können Muster in Daten entdecken und aus ihnen Lernen, um eigene Prognosen und Entscheidungen zu treffen.

In der Regel wird Machine Learning in folgende Teilbereiche untergliedert \cite{monkeylearnIntroductionMachine}:
\begin{itemize}
    \item \textbf{Supervised Learning:}
    Ein Modell wird anhand von gelabelten Trainingsdaten trainiert.
    Ein Datentupel besteht dabei aus einer Eingabe und der dazu gewollten Ausgabe.
    Der Algorithmus sucht beim Training nach Zusammenhängen und Abhängigkeiten um anschließend, die gelernten Daten generalisierend, Ausgaben für unbekannte Eingaben generieren zu können.
    Üblicherweise wird Supervised Learning für Regressions- und Klassifikationsprobleme eingesetzt.

    \item \textbf{Unsupervised Learning:}
    Wird in der Regel für Clusterbildung von unbeschrifteten Trainingsdaten verwendet.
    Der Algorithmus muss dabei selbstständig nach Mustern in den Daten suchen.
    Unsupervised Learning kann dabei helfen, Einblicke in große Datensätzen zu erhalten, um etwa versteckte Trends zu entdecken.

    \item \textbf{Semi-Supervised Learning:}
    Für das Training werden beim Semi-Supervised Learning sowohl ein kleiner gelabelter, als auch ein großer ungelabelter Datensatz verwendet.
    Dabei werden die Vorzüge von Supervised und Unsupervised Learning miteinander verbunden.
    Interessant ist dies vor allem bei sehr großen Datensätzen (zum Beispiel bei Bild-Klassifizierung), da die Labelung der Daten in der Regel manuell erfolgen muss.

    \item \textbf{Reinforcement Learning:}
    Beim Reinforcement Learning kann ein Agent Aktionen tätigen, für die er entweder belohnt oder bestraft wird.
    Es ist sein Ziel, selbstständig ein bestmögliches Verhalten zu lernen, um seine Belohnung zu maximieren.
    Dabei werden keine Trainingsdaten verwendet.
    Der Agent lernt ausschließlich aus seinen eigenen Erfahrungen und Fehlern.
    Reinforcement Learning findet vor allem in den Bereichen Robotik und Videospiele Einsatz und wird auch in dieser Arbeit verwendet.
\end{itemize}

% \begin{itemize}
%     \item Unterkategorie der künstlichen Intelligenz
%     \item ML ist ein automatisierter Prozess, der ...
%     \item ermöglicht Computern, eigenständig aus Trainingsdaten zu lernen und sich mit der Zeit zu verbessern, ohne explizit programmiert zu werden
%     \item ML Algorithmen können Muster in Daten entdecken und aus ihnen lernen, um eigene Voraussagen und Entscheidungen zu treffen
    
%     \item verschiedene Teilbereiche
%     \item Supervised Learning
    
%     Vorhersagen von gelabelten Trainingsdaten
%     Datensatz: Input mit gewolltem Output
%     \item Unsupervised Learning
    
%     Einblicke und Beziehungen in unbeschrifteten Daten
%     Modelle müssen selbstständig Muster finden
%     Clustering
%     kann helfen, versteckte Muster oder Trends zu entdecken
%     \item Semi-Supervised Learning
    
%     kleiner Teil gelabelter Daten, viele ungelabelte
%     besonders für große Datensätze interessant, Bild-Klassifizierung
%     \item Reinforcement Learning
    
%     vor allem im Bereich Robotik und Videospiele vertreten
%     Agent kann Aktionen tätigen, für die er belohnt oder bestraft wird
%     es ist sein Ziel, ein bestmögliches Verhalten zu lernen, um seine Belohnung zu maximieren
%     keine Trainingsdaten, Lernen aus eigenen Fehlern
%     in diesem Kontext besonders interessant wird verwendet

%     \item \cite{monkeylearnIntroductionMachine}
% \end{itemize}

\subsection{Reinforcement Learning}
Reinforcement Learning ist ein rechnerischer Ansatz, um zielorientierte Lern- und Entscheidungsprozesse zu verstehen und nachzubilden.
Wie oben schon beschrieben, steht dabei ein \emph{Agent} im Zentrum, der aus der direkten Interaktion mit seiner \emph{Umgebung (engl. Environment)} lernt, ohne dabei eine beispielhafte Anleitung oder vollständige Modelle der Umgebung zu benötigen.
Das formale Framework des \emph{\acf{mdp}} wird genutzt, um die Interaktion zwischen dem lernenden Agenten und seiner Umgebung zu definieren.
\acp{mdp} sind eine mathematisch idealisierte Form eines Reinforcement Learning Problems, für die präzise, theoretische Aussagen getroffen werden können \cite[13]{sutton2018rlintro}.
\autoref{fig:mdp} stellt die grundlegende Struktur eines Reinforcement Learning Problems dar.
Die einzelnen Bestandteile der Abbildung werden nachfolgend erläutert.

\begin{figure}
    \centering
    \includegraphics[width = 0.7\textwidth]{Bilder/MDP.pdf}
    \caption{Die Interaktion von Agent und Umgebung als \ac{mdp} \cite[48]{sutton2018rlintro}}
    \label{fig:mdp}
\end{figure}

Agenten haben explizite Ziele, können Aspekte ihrer Umgebung wahrnehmen und \emph{Aktionen} $A$ auswählen, um mit ihrer Umgebung zu interagieren und diese zu beeinflussen.
Es wird davon ausgegangen, dass Reinforcement Learning diejenige Strategie des Machine Learning ist, die dem natürlichen Lernen von Menschen und Tieren am nächsten kommt.
Viele zentrale Algorithmen des Reinforcement Learnings sind ursprünglich durch biologische Systeme inspiriert \cite[4]{sutton2018rlintro}.

Besonders wichtig für Reinforcement Learning ist das Konzept von \emph{Zuständen} $S$.
Ein Zustand kann dabei als eine Art Signal verstanden werden, das dem Agent Informationen über den Zustand der Umgebung liefert.
Weiterhin definieren folgende Elemente ein Reinforcement Learning Problem:
\begin{itemize}
    \item \textbf{Policy} $\mathbf{\pi}$\textbf{:}
    Die Policy definiert, wie sich der Agent zu einer gegebenen Zeit verhält.
    Sie stellt ein Mapping zwischen den wahrgenommenen Zuständen der Umgebung und den durchzuführenden Aktionen dar.
    Die Policy ist hinreichend, um das Verhalten des Agents zu bestimmen \cite[6]{sutton2018rlintro}.

    \item \textbf{Reward-Signal} $\mathbf{R}$\textbf{:}
    Das Reward-Signal definiert das Ziel eines Reinforcement Learning Problems.
    Bei jedem \emph{Zeitschritt} $t$ sendet die Umgebung ein Skalar an den Agent.
    Das einzige Ziel des Agents ist die Maximierung des kumulativen Rewards.
    Der Reward ist die primäre Basis für Änderungen an der Policy \cite[6]{sutton2018rlintro}.

    \item \textbf{Value-Funktion} $\mathbf{V}$\textbf{:}
    Die Value-Funktion legt fest, was auf lange Sicht gut ist.
    Der Value eines Zustands ist der kumulierte Reward, den ein Agent, ausgehend von diesem Zustand, in der Zukunft erwarten kann.
    Values geben die langfristige Attraktivität von Zuständen an.
    Die Wahl einer Aktion wird auf Basis der Value-Einschätzung des aktuellen Zustands getroffen.
    Im Vergleich zum Reward-Signal sind Values allerdings deutlich schwerer zu bestimmen, da diese anhand einer Sequenz von Observationen des Agenten geschätzt werden müssen \cite[6]{sutton2018rlintro}.

    \item \textbf{Modell der Umgebung (optional):}
    Manche Reinforcement Learning Systeme nutzen ein Modell der Umgebung.
    Dieses Modell erlaubt das Ziehen von Schlussfolgerungen über das Verhalten der Umgebung.
    So kann etwa eine Voraussage des nächsten Resultierenden Zustands und Rewards, ausgehend von einem gegebenen Zustand und einer Aktion getroffen werden.
    Genutzt werden diese Modelle zur Planung.
    Es werden also Entscheidungen für eine Folge von Aktionen auf Basis möglicher zukünftiger Situationen getroffen, bevor diese tatsächlich erlebt werden.
    Reinforcement Learning Methoden, die Modelle und Planung verwenden, werden als \emph{Modell-basiert} bezeichnet.
    Im Gegensatz dazu stehen \emph{Modell-freie} Methoden, welche explizit auf Basis von Trial-and-Error lernen \cite[7]{sutton2018rlintro}.
\end{itemize}

Beim Reinforcement Learning versucht der Agent, mit seinen ausgeführten Aktionen ein Reward-Signal zu maximieren.
Dabei muss ein Kompromiss zwischen Nutzen des Gelerntem und Entdecken von Neuem gefunden werden.
Es besteht das Dilemma, dass weder das eine, noch das andere uneingeschränkt verfolgt werden kann, ohne bei der Ausführung der Aufgabe zu scheitern, denn beim Entdecken muss der Agent auch schlechte Aktionen ausführen, für die er keinen Reward erhält.
Entdeckt er jedoch nichts, weiß er auch nicht, welche Aktionen einen hohen Reward erzeugen \cite[3]{sutton2018rlintro}.

% \begin{itemize}
%     \item Markov Decision Processes sind eine mathematisch idealisierte Form eines Reinforcement Learning Problems für die präzise theoretische Aussagen getroffen werden können
    
%     \item rechnerischer Ansatz, um zielorientierte Lern- und Entscheidungsprozesse zu verstehen
%     \item im Zentrum steht dabei ein Agent, der aus der direkten Interaktion mit seiner Umgebung lernt, ohne dabei eine beispielhafte Anleitung oder vollständige Modelle der Umgebung zu benötigen
%     \item nutzt das formale Framework des Markov Decision Processes um die Interaktion zwischen dem lernenden Agenten und seiner Umgebung zu definieren
%     \item Versuch, ein Reward-Signal zu maximieren
%     \item Kompromiss zwischen Nutzen und Entdecken
%     \item Dilemma: weder das eine, noch das andere kann uneingeschränkt verfolgt werden, ohne zu scheitern
%     \item Agenten haben explizite Ziele, können Aspekte ihrer Umgebung wahrnehmen und Aktionen auswählen, um ihre Umgebung zu beeinflussen
%     \item viele zentrale Algorithmen des Reinforcement Learning sind ursprünglich durch biologische Systeme beeinflusst
    
%     \item Konzept von Zuständen; ein Zustand ist eine Art Signal, das dem Agent Informationen über den Zustand der Umgebung liefert
    
%     Elemente:
%     \item Policy
    
%     definiert, wie sich der Agent zu einer gegebenen Zeit verhält
%     Mapping zwischen wahrgenommenen Zuständen der Umgebung und durchzuführenden Aktionen
%     hinreichend um das Verhalten zu bestimmen
%     \item Reward-Signal
    
%     definiert das Ziel eines Reinforcement Learning Problems
%     bei jedem Zeitschritt sendet die Umgebung eine einzelne Nummer an den Agent
%     das einzige Ziel des Agents ist die Maximierung des kommulativen Rewards
%     primäre Basis für Änderungen an der Policy
%     \item Value-Funktion
    
%     legt fest, was auf lange Sicht gut ist
%     Value eines Zustands ist der kommulierte Reward, den ein Agent, ausgehend von diesem Zustand, in der Zukunft erwarten kann
%     Values geben die langfristige Attraktivität eines Zustands an
%     die Wahl einer Aktion wird auf Basis der Value-Einschätzung getroffen
%     aber, Values sind deutlich schwerer zu bestimmen als Rewards
%     \item optional Modell der Umgebung
    
%     manche Reinforcement Learning Systeme nutzen ein Modell der Umgebung
%     erlaubt das Ziehen von Schlussfolgerungen über das Verhalten der Umgebung
%     etwa: Voraussagen des nächsten resultierenden Zustands und Rewards ausgehend von einem Zustand und einer Aktion
%     werden zur Planung genutzt, also Entscheiden für eine Folge von Aktionen auf Basis möglicher zukünftiger Situationen bevor diese tatsächlich erlebt werden
%     Reinforcement Learning Methoden, die Modelle und Planung verwenden, werden als modell-basiert bezeichnet
%     im Gegensatz dazu stehen modell-freie Methoden, also explizites Lernen auf Basis von trial-and-error 
% \end{itemize}
% Begriffsdefinitionen
% Markov Decision Process


\subsection{Deep Reinforcement Learning}
Wie auch andere Algorithmen haben Reinforcement Learning Algorithmen Skalierungsprobleme hinsichtlich ihrer Komplexität.
So kommt es beispielsweise zu Schwierigkeiten, die Value- oder die Policy-Funktion abzubilden, wenn die Dimension des Reinforcement Learning Problems zu groß wird.
Insbesondere bei hoch-dimensionalen, kontinuierlichen Zustands- und Aktionsräumen ist dies der Fall.

Der wichtigste Bestandteil von Deep Learning sind Deep Neural Networks.
Diese Netzwerke können automatisch kompakte, niedrig-dimensionale Repräsentationen von hoch-dimensionalen Daten finden.
Beim Deep Reinforcement Learning werden Algorithmen und Technologien des Deep Learning in Reinforcement Learning eingebracht.
Dabei werden Deep Neural Networks als Funktionsapproximatoren für die Value-Funktion oder die Policy verwendet.
Diese neue Kombination macht eine Skalierung auf bislang unlösbare Entscheidungsprobleme möglich \cite{Arulkumaran2017}.

% \begin{itemize}
%     \item RL Algorithmen, wie andere Algorithmen auch haben Problem mit Komplexität: Speicherkomplexität, Rechenkomplexität und Probenkomplexität (letztere ML spezifisch)
%     \item Deep Learning, basierend auf den mächtigen Funktionsapproximationen und repräsentativen Lerneigenschaften von Deep Neural Networks, bietet neue Tools, um gegen diese Probleme anzukommen
%     \item wichtigste Eigenschaft von Deep Learning: Deep Neural Networks können automatisch kompakte, niedrig-dimensionale Repräsentationen von hoch-dimensionalen Daten finden
%     \item grundsätzlich eigentlich nur Deep Learning Algorithmen im Kontext von RL
%     \item macht Skalierung auf bislang unlösbare Entscheidungsprobleme möglich (z.B. hoch-dimensionale Zustands- und Aktionsräume)
%     \item 
% \end{itemize}


\section{Unity3D}
Unity3D ist eine plattformübergreifende Game Engine, die erstmal 2005 angekündigt wurde.
Primärer Zweck der Unity Engine ist die Entwicklung von Videospielen für Computer, Konsolen und Mobilgeräte.
Dabei ist Unterstützung für zwei- und dreidimensionale Grafik enthalten.
VR Entwicklung ist ebenso möglich.
Das Scripting innerhalb der Engine erfolgt primär in C\# \cite{freecodecamp.unityIntroduction}.
Neben dem Einsatz in der Spieleentwicklung ist Unity jedoch auch für den Einsatz in anderen Branchen geeignet, so zum Beispiel in der Architektur oder der Forschung \cite[30]{waidner.2020}, wo mit Unity Simulationen der realen Welt erstellt werden können.
Als Grafik-APIs werden unter anderem Direct3D (Windows), OpenGL (Linux, macOS, Windows) und WebGL unterstützt.
Unity enthält einen Asset Store für die Entwickler-Community, über den Dritten das Hoch- und Herunterladen kommerzieller und freier Ressourcen (zum Beispiel Texturen, Modelle und Plugins) ermöglicht wird \cite{freecodecamp.unityIntroduction}.
Der Einsatz von Unity für Projekte mit weniger als 100.000 \$ jährlichem Gewinn ist kostenlos \cite{unityPersonal}.

% cross-plattform Game Engine
% 2005 angekündigt
% primär für die Entwicklung von Videospielen und Simulationen für Computer, Konsolen und Mobilgeräten
% unterstützt 2D und 3D Grafik und C\# Skripting
% auch für VR Entwicklung geeignet
% Grafik APIs: unter anderem Direct3D (Windows), OpenGL (Linux, macOS, Windows), WebGL
% Asset Store für Entwickler Community, Upload \& Download für kommerzielle und freie Ressourcen Dritter (Texturen, Modelle, Plugins)
% \cite{freecodecamp.unityIntroduction}
% wird auch außerhalb des Spielebereichs verwendet, so auch für Architektur und Forschung \cite[30]{waidner.2020}
% für nicht wirtschaftliche Projekte kostenlos (Quelle Unity Projektseite)

\subsection{Unity Machine Learning Agents Toolkit}
Das Unity Machine Learning Agents Toolkit (kurz \emph{ML-Agents}) ist ein von Unity Technologies entwickeltes, quelloffenes Projekt, welches 2017 erstmals als Testversion veröffentlicht wurde, seitdem sehr aktiv weiterentwickelt wird und inzwischen die Produktreife erreicht hat.
ML-Agents ermöglicht es Spiele und Simulationen, als Trainingsumgebung für intelligente Agenten zu dienen.
Dabei werden State-of-the-Art, PyTorch-basierte Implementierungen gängiger Machine Learning Algorithmen angeboten, um ein einfaches Training mit möglichst geringer Einstiegshürde zu ermöglichen.
Alternativ können auch eigene Algorithmen zum Training verwendet werden.
Wie Unity selbst auch ist ML-Agents für den Einsatz in 2D-, 3D- und VR/AR-Umgebungen geeignet.
Als Trainingsmethoden werden unter anderem Reinforcement Learning, Imitation Learning und Neuroevolution unterstützt \cite{mlagentsDocHome}.

% - quelloffenes Projekt, 2017 veröffentlicht
% - ermöglicht Spielen und Simulationen, als Trainingsumgebung für intelligente Agents zu dienen
% - bietet state-of-the-art, PyTorch-basierte Implementierungen gängiger Machine Learning Algorithmen, um ein einfaches Training mit möglichst geringer Einstiegshürde zu ermöglichen
% - es können auch eigene Algorithmen zum Training verwendet werden
% - unterstützt werden auch hier 2D, 3D und VR/AR
% - als Methoden für das Training werden unter anderem Reinforcement Learning, Imitation Learning und Neuroevolution unterstützt
% \cite{mlagentsDocHome}

ML-Agents besteht aus folgenden high-level Komponenten \cite{mlagentsOverview} (siehe \autoref{fig:learning-environment-basic}):
\begin{itemize}
    \item \textbf{Trainingsumgebung (Learning Environment):}
    Die Trainingsumgebung enthält eine Unity Szene und sämtliche Game Charaktere.
    Die Unity Szene stellt dabei die Umgebung bereit, in der Agenten ihre Beobachtungen machen, handeln und lernen.
    Mithilfe des ML-Agents Unity SDK kann jede Unity Szene in eine Trainingsumgebung transformiert werden, indem GameObjects als Agenten definiert werden.

    \item \textbf{Python Low-Level API:}
    Die Python API enthält ein low-level Python Interface, welches die Aufgabe besitzt, mit der Trainingsumgebung zu interagieren und diese zu manipulieren.
    Diese Python API ist im Gegensatz zur Trainingsumgebung kein Teil von Unity, sondern kommuniziert mit Unity durch den Communicator.

    \item \textbf{Externer Communicator:}
    Der Communicator erfüllt die Aufgabe, die Python API mit der Trainingsumgebung zu verbinden.

    \item \textbf{Python Trainer:}
    In den Python Trainern sind alle Machine Learning Algorithmen enthalten, die ein Training der Agenten ermöglicht.
    Dieses Paket stellt die zum Training genutzte \ac{cli} (\code{mlagents-learn}) bereit.
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width = 0.5\textwidth]{Bilder/ml-agents/learning_environment_basic.png}
    \caption{Vereinfachtes Block-Diagramm des ML-Agents-Toolkits \cite{mlagentsOverview}}
    \label{fig:learning-environment-basic}
\end{figure}

Die Trainingsumgebung wird durch zwei enthaltene Unity-Komponenten organisiert \cite{mlagentsOverview}.
\begin{itemize}
    \item \textbf{Agents} sind an ein Unity GameObject geknüpft (beliebiger Charakter innerhalb einer Szene).
    Sie sind gleichzusetzen mit dem Agent eines Reinforcement Learning Problems.
    Agents generieren die Observations (Beobachtungen) des GameObjects, welche dem Reinforcement Learning Algorithmus zugeführt werden, führen die vom Algorithmus empfangenen Aktionen aus und weisen den Reward zu.
    Jeder Agent ist mit einem Behavior verknüpft.

    \item \textbf{Behaviors} (Verhalten) definieren Attribute des Agenten, so auch die Anzahl der Aktionen, die der Agent entgegennehmen kann.
    Ein Behavior kann als Funktion verstanden werden, welche Observations und Reward des Agents als Eingabeparameter enthält und auszuführende Aktionen zurückliefert.
    Behaviors werden in drei Typen unterschieden: \emph{Learning}, \emph{Heuristic} und \emph{Inference}.
    Learning Behaviors sind noch nicht definiert, können aber trainiert werden.
    Heuristic Behaviors werden mittels manuell implementierter Regeln im Quellcode definiert.
    Inference Behavior werden von trainierten Neural-Network-Dateien (entsprechen der finalen, trainierten Policy) repräsentiert.
    Nachdem ein Learning Behavior trainiert wurde, wird es zum Inference Behavior.
\end{itemize}
Herausstellenswert hierbei ist, dass ML-Agents die Möglichkeit bietet, mehrere Agents in einer Trainingsumgebung zu platzieren, die jedoch mit demselben Behavior verknüpft sein können.
Dies kann dafür genutzt werden, das Training zu parallelisieren und damit zu beschleunigen.

\subsection{Reinforcement Learning Algorithmen}
Von ML-Agents werden zwei Trainingsalgorithmen bereitgestellt, die sich dem (Deep) Reinforcement Learning zuordnen lassen.
Dies sind \acf{ppo} und \acf{sac} \cite{mlagentsOverview}.
In \cite{waidner.2020} wurden diese Algorithmen bereits verglichen.
\ac{ppo} ist der Standardalgorithmus von ML-Agents, da er sich, verglichen mit vielen anderen Reinforcement Learning Algorithmen, als für den allgemeinen Einsatz besser geeignet gezeigt hat \cite{schulman2017proximal,openaiPPO}.
\ac{ppo} ist ein on-policy Algorithmus.
Das bedeutet, dass in jeder Iteration des Lernvorgangs nur aus Erfahrungen gelernt wird, die mit der aktuellen Version der Policy gesammelt wurden.
\ac{sac} hingegen ist ein off-policy Algorithmus und lernt somit aus allen Erfahrungen, die er jemals während des gesamten Trainingsvorgangs gesammelt hat \cite{suran2020,sagar2020}.
Daraus ergeben sich für beide Algorithmen unterschiedliche Vor- und Nachteile.
On-policy Algorithmen haben in der Regel einen deutlich stabileren Lernfortschritt, als off-policy Algorithmen.
Andererseits brauchen on-policy Algorithmen in der Regel deutlich mehr Trainingsschritte, um nennenswerte Ergebnisse zu erzielen \cite{mlagentsOverview}.
Im Zuge der Vorgängerarbeit wurde mit beiden Algorithmen gearbeitet, mit dem Ergebnis, dass der \ac{ppo}-Algorithmus auch für das konkrete Problem im Rahmen dieser Studienarbeit deutlich bessere Resultate liefert \cite[48]{waidner.2020}.

\subsection{Training und Hyperparameter}
\label{sec:training}
Wenn die Trainingsumgebung erstellt ist, kann mit dem Training der Agenten begonnen werden.
Der Einstiegspunkt hierzu ist immer die \ac{cli} \code{mlagents-learn}, welche Teil der Python-Umgebung von ML-Agents ist.
Das Training kann dann entweder direkt im Unity-Editor oder mittels einer kompilierten Umgebung erfolgen.
Falls letztere verwendet werden soll, kann sie mittels des Parameters \code{--env=<env\_name>} spezifiziert werden.
Einem Training sollte in der Regel auch ein Run-Identifier gegeben werden (\code{--run-id=<run-identifier>}).
Über diesen können die Ergebnisse besser zugeordnet werden und es ist beispielsweise auch möglich, ein unterbrochenes Training wiederaufzunehmen \cite{mlagentsTraining}.

Optional kann das Training mithilfe einer YAML-Datei konfiguriert werden.
In dieser können sowohl allgemeine Aspekte des Trainingsvorgangs festgelegt werden (etwa wie viele Trainingsschritte durchgeführt werden sollen), als auch Hyperparameter eingestellt werden, die spezifisch für das jeweils durchzuführende Training sind.
Für ein Training mit dem \ac{ppo}-Algorithmus sind vor allem die folgenden Hyperparameter von Bedeutung \cite{mlagentsHyperparameter}:
\begin{itemize}
    \item \textbf{batch\_size:} Anzahl der Erfahrungen in jeder Iteration des Gradientenabstiegs.
    \item \textbf{buffer\_size:} Anzahl der Erfahrungen, die gesammelt werden, bevor das Policy Modell aktualisiert wird, also wie viele Erfahrungen gesammelt werden, bevor gelernt wird.
    \item \textbf{learning\_rate:} Initiale Lernrate für den Gradientenabstieg.
    Damit kann gesteuert werden, wie schnell das Modell am Anfang lernt.
    Sollte so groß wie möglich gewählt werden, um ein schnelles Training durchzuführen.
    Wenn das Training jedoch instabil verläuft, sollte dieser Wert verringert werden.
    \item \textbf{learning\_rate\_schedule:} Gibt an, wie die Lernrate über die Zeit verändert wird.
    Für \ac{ppo} wird diese in der Regel linear verringert, damit das Training möglichst stabil konvergiert.
    \item \textbf{beta:} Faktor um die Entropie des Trainingsprozesses zu regulieren.
    Die Entropie sollte gegenläufig zum Reward langsam im Laufe des Trainingsprozesses fallen.
    Mit einer Erhöhung des beta-Werts wird die Entropie länger auf einem höheren Level gehalten und umgekehrt.
    \item \textbf{epsilon:} Begrenzt die Veränderung der Policy während des Trainingsprozesses.
    Wird epsilon klein gewählt, so werden Änderungen an der Policy kleinschrittiger vorgenommen, was das Training stabilisiert, aber auch mehr Trainingsschritte benötigt.
    \item \textbf{lambd:} Parameter zur Regularisierung während der Berechnung des \enquote{Generalized Advantage Estimate}.
    Prinzipiell gibt der Parameter an, wie stark der Agent auf seine aktuelle Schätzung des Values aufbaut, während die Schätzung des Values aktualisiert wird.
    Bei einem niedrigen Wert von lambd liegt das Gewicht beim aktuell geschätzten Value (der einen hohen Bias haben kann) und bei einem hohen Wert werden die eigentlichen Rewards höher gewichtet (welche jedoch einer hohen Varianz unterliegen können).
    \item \textbf{num\_epoch:} Wie viele Durchläufe durch die gesammelten Erfahrungen gemacht werden können, wenn ein Gradientenabstieg durchgeführt wird.
    Ein kleiner Wert führt zu stabilerem, aber auch langsamerem Training.
    \item \textbf{gamma:} Diskontinuierungsfaktor für zukünftige Rewards.
    Gibt an, wie weit der Agent bei Spekulation auf mögliche Rewards in die Zukunft \enquote{denken} soll.
    \item \textbf{hidden\_units:} Anzahl der Neuronen in den Hidden Layers des zu trainierenden Neural Network.
    Sollte mit der Komplexität des Problems nach oben skaliert werden.
    \item \textbf{num\_layers:} Anzahl der Hidden Layers des Neural Networks.
    Analog zur Anzahl der Neuronen sollte dieser Wert mit der Komplexität des Problems skaliert werden.
    Weniger Layer trainieren in der Regel schneller, können aber unter Umständen zur Abbildung komplexer Probleme nicht ausreichen.
    \item \textbf{normalize:} Gibt an, ob die eingegebenen Observation Vektoren normalisiert werden.
    Bei Problemen mit komplexen, kontinuierlichen Beobachtungs- und Aktionsräumen kann dies hilfreich sein.
\end{itemize}

Die in der Regel verwendeten Befehle zur Initiierung des Trainings folgen grundlegend folgendem Aufbau \cite{mlagentsTraining}: \\
\code{mlagents-learn <yaml-config> --env=<env\_name>  --run-id=<run-id>}

\subsection{Bewertungskriterien}
\label{sec:bewertung}
Der Verlauf des Trainings kann mittels des Tools \emph{Tensorboard} überwacht werden.
Tensorboard startet dabei einen lokalen Webserver, der Auswertungen der Trainingsdaten im Browser darstellt.
Um das Ergebnis eines Trainings objektiv zu bewerten, muss natürlich das eigentliche, resultierende Modell betrachtet werden, jedoch geben auch einige der in Tensorboard dargestellten Metriken frühzeitig Aufschluss über den potenziellen Erfolg des Trainings.
Allen voran steht der \emph{Cumulative Reward}.
Dieser Wert gibt den mittleren kumulierten Reward innerhalb einer Episode von allen Agenten an.
Natürlich hängt der Verlauf dieser Kurve stark davon ab, wie die individuelle Reward-Funktion gewählt wird, allerdings sollte sich dieser Wert bei einem erfolgreichen Training stetig erhöhen und keine starken Schwankungen aufweisen.
Die \emph{Episodenlänge} zeigt an, wie lange die Episoden im Mittel gedauert haben.
Wird die Umgebung etwa beim unwiderruflichen Scheitern eines Agenten zurückgesetzt, so kann hier abgelesen werden, wie lange es im Mittel dauert, bis es zu einem fatalen Scheitern kommt.
\emph{Policy Loss} hängt davon ab, wie stark die Policy sich verändert.
Der Betrag dieser Funktion sollte im Laufe des Trainings abnehmen, wenn die Policy zu einer stabilen Funktion konvergiert.
Schließlich gibt \emph{Value Loss} an, wie stark die vorhergesagten Values von den tatsächlich erhaltenen Rewards abweichen.
Bei einem erfolgreichen Training sollte dieser Wert zu Beginn ansteigen und dann im Anschluss gegen einen niedrigen Wert konvergieren \cite{aurelian2018,untiyMetrics}.

Weiterhin werden in Tensorboard die Hyperparameter im Laufe der Zeit dargestellt.
Entsprechend den Beschreibungen in \autoref{sec:training} können auch diese Werte live mitverfolgt und gegebenenfalls korrigiert werden.


\section{Vektorgeometrie}
Da Vektoren im dreidimensionalen Raum ein essenzieller Bestandteil der Arbeit mit Unity sind, sollen die dafür wichtigen Konzepte hier kurz vorgestellt werden.

Grundsätzlich gesehen ist ein Vektor ein mathematisches Konstrukt und hat eine Länge und eine Richtung.
Im Gegensatz zu Punkten geben Vektoren keinen festen Ort an, sondern beschreiben den Weg von einem Punkt zu einem anderen \cite[6]{kohn2012}.
Dabei besitzt ein Vektor für jede Dimension eine Komponente.
Trotzdem ist eine Ortsangabe mit einem Vektor möglich (und wird in Unity für die GameObjects verwendet).
Dieses Konstrukt besteht aus einem Vektor, der an einem bestimmten Punkt beginnt und nennt sich Ortsvektor \cite[21]{kohn2012}.
Im Weiteren Verlauf wird davon ausgegangen, dass Ortsvektoren dem Koordinatenursprung entstammen.

\autoref{fig:weg} stellt die Ortsvektoren $\overrightarrow{a}$ und $\overrightarrow{b}$ dar, welche vom Koordinatenursprung auf die Punkte $A$ und $B$ zeigen.
Der Vektor $\overrightarrow{AB}$ stellt den Vektor dar, der von $A$ zu $B$ führt.
Addiert man mehrere Vektoren, so erhält man den daraus resultierenden Vektor \cite[11]{kohn2012}.
Wie in \autoref{eqn:weg} ersichtlich ist, kann durch Addition von $\overrightarrow{a}$ und $\overrightarrow{AB}$ wiederum $\overrightarrow{b}$ bestimmt werden.
Mittels eines Umstellens der Gleichung kann aber auch aus den beiden Ortsvektoren der Verbindungsvektor $\overrightarrow{AB}$ bestimmt werden \cite[12]{kohn2012}.

\begin{figure}
    \centering
    \begin{tikzpicture}
        \coordinate[label=right:$A$] (A) at (3,2.5);
        \coordinate[label=left:$B$] (B) at (2,5);

        \draw[-latex] (0,0) -- node[below] {$\overrightarrow{a}$} (A);
        \draw[-latex] (0,0) -- node[left] {$\overrightarrow{b}$} (B);
        \draw[-latex, dashed] (A) -- node[right] {$\overrightarrow{AB}$} (B);
    \end{tikzpicture}
    \caption{Vektor von Punkt $A$ zu Punkt $B$}
    \label{fig:weg}
\end{figure}

Für die Länge eines Vektors (auch Norm genannt), gibt es verschiedene Definitionen.
Die hier relevante und erklärte ist die sogenannte \emph{euklidische Norm}.
Zur Bestimmung dieser wird die Summe aus den Quadraten der Komponenten gebildet und aus dieser Summe im Anschluss die Quadratwurzel gezogen \cite[30]{kohn2012} (\autoref{eqn:laenge3d}).
Der Hintergrund liegt hierbei im Satz des Pythagoras, was ersichtlich wird, wenn man sich die Zerlegung eines zweidimensionalen Vektors in seine einzelnen Komponenten anschaut (\autoref{fig:zerlegung}, \autoref{eqn:laenge2d}).


\begin{equation}
    \label{eqn:weg}
    \overrightarrow{a} + \overrightarrow{AB} = \overrightarrow{b}
    \quad \Rightarrow \quad
    \overrightarrow{AB} = \overrightarrow{b} - \overrightarrow{a}
\end{equation}

\begin{figure}
    \centering
    \begin{tikzpicture}
        \draw[-latex, dashed] (0,0) -- node[below] {$\overrightarrow{x}$} (5,0);
        \draw[-latex, dashed] (5,0) -- node[right] {$\overrightarrow{y}$} (5,5);
        \draw[-latex] (0,0) -- node[left] {$\overrightarrow{v}$} (5,5);
    \end{tikzpicture}
    \caption{Zerlegung des zweidimensionalen Vektors $\overrightarrow{v}$}
    \label{fig:zerlegung}
\end{figure}

\begin{equation}
    \label{eqn:laenge2d}
    \overrightarrow{v} = \begin{pmatrix}x \\ y\end{pmatrix}
    \quad \Rightarrow \quad
    |\overrightarrow{v}| = \sqrt{x^2 + y^2}
\end{equation}

\begin{equation}
    \label{eqn:laenge3d}
    \overrightarrow{v} = \begin{pmatrix}x \\ y \\ z\end{pmatrix}
    \quad \Rightarrow \quad
    |\overrightarrow{v}| = \sqrt{x^2 + y^2 + z^2}
\end{equation}

Dadurch, dass Vektoren eine Richtung darstellen, ist es ebenfalls möglich, den Winkel $\beta$ zu bestimmen, der zwischen diesen Vektoren liegt, wenn man sie aufeinander stellt.
Dafür wird zunächst das sogenannte Skalarprodukt der beiden Vektoren gebildet \cite[45]{kohn2012} (\autoref{eqn:skalarprodukt}).
Anschließend wird das Skalarprodukt durch das Produkt der Längen beider Vektoren geteilt und in die $\arccos$ Funktion eingesetzt \cite[60]{kohn2012} (\autoref{eqn:zwischenwinkel}).

\begin{equation}
    \label{eqn:skalarprodukt}
    \overrightarrow{v} \cdot \overrightarrow{w} =
    \begin{pmatrix}v_1 \\ v_2 \\ v_3\end{pmatrix} \cdot \begin{pmatrix}w_1 \\ w_2 \\ w_3\end{pmatrix} =
    v_1 \cdot w_1 + v_2 \cdot w_2 + v_3 \cdot w_3
\end{equation}

\begin{equation}
    \label{eqn:zwischenwinkel}
    \beta = \arccos(\frac{\overrightarrow{v} \cdot \overrightarrow{w}}{|\overrightarrow{v}| \cdot |\overrightarrow{w}|})
\end{equation}


\section{Beschreibung der Projektbasis}
Die Zielsetzung dieser Arbeit soll aufbauend auf einer Vorgängerarbeit \cite{waidner.2020} realisiert werden, welche vor einigen Jahren ebenfalls im Rahmen einer Studienarbeit durchgeführt wurde.
Hier soll nun zunächst die Vorgehensweise der Vorgängerarbeit in ihren Grundzügen erläutert werden.

\subsection{Aufbau und Simulation des Roboters}
Kernelement der Arbeit ist ein vierbeiniger, 3D-gedruckter Roboter, der in seiner Anatomie einer Spinne gleicht.
Der Roboter besteht aus einer rechteckigen Zentralplatte.
An jeder Ecke dieser Platte ist ein Bein angebracht.
Die Beine des Roboters bestehen jeweils aus drei separaten Teilen.
Folglich hat jedes Bein zwei Gelenke \cite[52]{waidner.2020}.
Jedes dieser Gelenke wird mit einem Servomotor des Typs SG90 XY realisiert, welche sich in einem Aktionsradius von 180° bewegen können.
Die Neutralstellung der Beine ist die jeweilige Mittelposition des Servomotors.
Angegeben wird der Aktionsradius des Servos als Wert im Bereich 0° - 180°, die Mittelstellung befindet sich also bei 90° \cite[38]{waidner.2020}.
Die verwendeten Servomotoren bewegen sich nur auf einer Achse.
An der Stelle, an der die Beine mit dem Körper verbunden sind, befindet sich ein weiterer Servomotor.
Mithilfe der verbauten Servomotoren ist es möglich, jedes Bein anzuziehen oder auszustrecken und es nach vorne beziehungsweise hinten zu bewegen.

Angesteuert werden die Servomotoren mit einem ESP8266 Mikrocontroller, welcher mittels I²C Steuersignale an ein Servo-Breakout-Board sendet, an welchem wiederum die Servomotoren angeschlossen sind.
Die Stromversorgung erfolgt über eine Batterie, damit sich der Roboter autark von einer Stromquelle im Raum bewegen kann \cite[54]{waidner.2020}.
Die aufgelisteten Bauteile werden so mit Kabelbindern an der Zentralplatte befestigt, dass sie nicht mit der Bewegungsfreiheit der Beine interferieren.
Am Roboter ist keinerlei Sensorik verbaut \cite[36]{waidner.2020}.

Da das Training des Roboters in der Realität zu lange dauert und dabei außerdem die Gefahr droht, den Roboter zu beschädigen, wurde eine Simulationsumgebung aufgebaut, in der das Training des Roboters erfolgt.
Diese ist in Unity realisiert.
In Unity ist es möglich, dasselbe 3D-Modell zu importieren, das auch für den Druck der Teile des Roboters verwendet wird, was eine akkurate Umsetzung der Dimensionen sicherstellt.

Die größte Schwierigkeit einer Simulation des Roboters besteht darin, die Servomotoren abzubilden.
Im Gegensatz zu einem Computerprogramm ist es Bauteilen in der realen Welt nicht möglich, instantan einen gezielten Zustand anzunehmen.
Das heißt, wird für einen Servomotor ein neuer Winkel vorgegeben, so braucht dieser eine bestimmte Zeit, bis er diesen erreicht hat.
Diese Zeit ist einerseits von der Bewegungsgeschwindigkeit des Servomotors abhängig, andererseits stellt die Last, die der Motor bewegt, einen weiteren Einflussfaktor dar.
Die Vorgängerarbeit implementiert eine Software-Simulation der Servomotoren, bei denen zumindest der Aspekt der mittleren Bewegungsgeschwindigkeit berücksichtigt wird, sowie verwandte, allgemeine Charakteristika der Bewegung \cite[37]{waidner.2020}.
Zusätzlich werden in Unity die ungefähren Gewichte der einzelnen Teile eingetragen, um eine akkurate Simulation der physikalischen Kräfte zu ermöglichen, die auf den Roboter einwirken.
Die Simulation dieser allgemeinen Physik wird bereits als Grundfunktion der Unity-Engine bereitgestellt und muss nicht separat implementiert werden.

% \begin{itemize}
%     \item Roboter aus 3D gedruckten Teilen
%     \item 4 Beine mit 2 Teilen, d.h. 2 Gelenken pro Bein, eins im Bein und eins am Körper
%     \item Gelenke sind Servos mit 120° Aktionsradius
%     \item Neutralstellung ist Mitte des Servos, also 60°
%     \item Steuerung über ESP8266 und Servo Breakout Board
%     \item Stromversorgung über Batterie
%     \item keine weitere Sensorik
    
%     \item Simulation dieses Roboters in Unity
%     \item dazu direkter Import des 3D Modells
%     \item Software-Simulation der Servos und deren Charakteristika (Bewegungsgeschwindigkeit)
%     \item ungefähres Gewicht der einzelnen Teile hinterlegen für Physiksimulation
% \end{itemize}

\subsection{Training des Roboters}
Mit der fertig aufgebauten Simulation kann nun der Roboter für alle erdenklichen Szenarien trainiert werden.
Dazu müssen Rahmenbedingungen des Trainings definiert werden.
Primär sind dies die Beobachtungen, die der Trainingsalgorithmus macht, die Aktionen, die er ausführen kann und die Belohnung, die er als Feedback für seine Handlungen erhält.
Die Zielsetzung der Vorgängerarbeit besteht aus einer reinen Vorwärtsbewegung.
Da der reale Roboter über keine Sensorik verfügt und lediglich die Ansteuerwinkel seiner Servomotoren kennt, sind dies auch die einzigen Beobachtungen, die dem Algorithmus zugänglich gemacht werden (kontinuierlicher Beobachtungsraum).
Als Aktionen kann der Algorithmus beliebige Zielwinkel für die Servomotoren setzen (kontinuierlicher Aktionsraum).
Die Belohnung für den Roboter besteht in der Vorgängerarbeit aus der Streckendifferenz, die er nach vorne zurückgelegt hat.
Bestraft wird der Roboter für ein Umkippen, um zu verhindern, dass bei der späteren Übertragung auf den realen Roboter die Steuerelektronik beschädigt wird.
Um das Training zu beschleunigen, sind in der Vorgängerarbeit mehrere Agenten nebeneinander in derselben Umgebung instanziiert.
Nach einer Einstellung der Hyperparameter wurden Modelle mit den Reinforcement-Learning-Algorithmen \ac{sac} und \ac{ppo} trainiert.
Dabei wurde festgestellt, dass in diesem Anwendungsfall die Ergebnisse des \ac{ppo}-Algorithmus denen des \ac{sac}-Algorithmus deutlich überlegen sind \cite[48]{waidner.2020}.

Die Bewegungsart, die der Roboter sich bei den vorgegebenen Trainingsbedingungen antrainiert, ist keine klassische Form des natürlichen Laufens.
Stattdessen führen die Ergebnisse des Trainings dazu, dass der Algorithmus den Roboter mit einem der Hinterbeine einknicken lässt und ihn dann sprungartig nach vorne katapultiert \cite[51]{waidner.2020}.

\subsection{Übertragung in die Realität}
Die Implementierung der Vorgängerarbeit sieht auch eine Übertragung der Ergebnisse auf den realen Roboter vor.
Zu diesem Zwecke können die Steuersignale, die der Roboter in der Simulation erhält, über eine serielle Verbindung übertragen werden und direkt auf dem realen Roboter angewandt werden.

Die unternommenen Versuche waren jedoch leider nicht erfolgreich.
Als Ursache fällt die Vermutung auf Hardwarefehler, da der Roboter -- in der Luft gehalten -- die Bewegungen korrekt nachahmt.
Wird der Roboter jedoch auf den Boden gestellt, knickt er unter seinem Gewicht sofort ein und kann die gelernte Laufmethodik nicht anwenden \cite[58]{waidner.2020}.
